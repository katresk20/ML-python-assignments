{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Boosting?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The term ‘Boosting’ refers to a family of algorithms which converts weak learner to strong learners. Boosting is an ensemble method for improving the model predictions of any given learning algorithm. The idea of boosting is to train weak learners sequentially, each trying to correct its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tHow do boosting and bagging differ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) Bagging technique can be an effective approach to reduce the variance of a model, to prevent over-fitting and to increase      the accuracy of unstable models. On the other hand, Boosting enables us to implement a strong model by combining a number of    weak models together.\n",
    "2) On the other hand, Boosting enables us to implement a strong model by combining a number of weak models together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhat are week and strong classifiers?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Weak classifiers (or weak learners) are classifiers that perform only slightly better than a random classifier. These are thus classifiers that have some clue on how to predict the right labels, but not as much as strong classifiers have like, e.g., Naive Bayes, Neural Network or SVM.\n",
    "Ada-boost classifier combines the weak classifier algorithm to form a strong classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhy are trees deemed fit for boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tExplain the step by step implementation of ADA Boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are pseudo residuals?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The initial guess of the Gradient Boosting algorithm is to predict the average value of the target y . ... For the variable x1 , we compute the difference between the observations and the prediction we made. This is called the pseudo-residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tExplain the step by step implementation of Gradient boosted trees."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Following are the steps of Gradient boosted.\n",
    "1) Calculate the average of the target label\n",
    "2) Calculate the residuals\n",
    "3) Construct a decision tree\n",
    "4) Predict the target label using all of the trees within the ensemble\n",
    "5) Compute the new residuals\n",
    "6) Repeat steps 3 to 5 until the number of iterations matches the number specified by the hyperparameter (i.e. number of          estimators)\n",
    "7) Once trained, use all of the trees in the ensemble to make a final prediction as to the value of the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tExplain the step by step implementation of XGBoost Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat are the advantages of XGBoost?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "XGBoost is an efficient and easy to use algorithm which delivers high performance and accuracy as compared to other algorithms.\n",
    "Following are the advnatages of XGboost:-\n",
    "\n",
    "1. Regularization: XGBoost has in-built L1 (Lasso Regression) and L2 (Ridge Regression) regularization which prevents the model from overfitting.\n",
    "\n",
    "2. Parallel Processing: XGBoost utilizes the power of parallel processing and that is why it is much faster than GBM.\n",
    "\n",
    "3. Cross Validation: XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
    "\n",
    "4. Effective Tree Pruning: A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
