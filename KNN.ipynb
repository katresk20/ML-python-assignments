{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tExplain the working of KNN algorithm.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems\n",
    "\n",
    "\n",
    "K-nearest neighbors (KNN) algorithm uses ‘feature similarity’ to predict the values of new datapoints which further means that the new data point will be assigned a value based on how closely it matches the points in the training set. We can understand its working with the help of following steps −\n",
    "\n",
    "Step 1 − For implementing any algorithm, we need dataset. So during the first step of KNN, we must load the training as well as test data.\n",
    "\n",
    "Step 2 − Next, we need to choose the value of K i.e. the nearest data points. K can be any integer.\n",
    "\n",
    "Step 3 − For each point in the test data do the following −\n",
    "\n",
    "      −> Calculate the distance between test data and each row of training data with the help of any of the method namely:              Euclidean, Manhattan or Hamming distance. The most commonly used method to calculate distance is Euclidean.\n",
    "\n",
    "      −> Now, based on the distance value, sort them in ascending order.\n",
    "\n",
    "      −> Next, it will choose the top K rows from the sorted array.\n",
    "\n",
    "      −> Now, it will assign a class to the test point based on most frequent class of these rows.\n",
    "\n",
    "Step 4 − End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tHow KNN predicts the output for regression and classification problems?\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-> KNN for Regression\n",
    "  When KNN is used for regression problems the prediction is based on the mean or the median of the K-most similar instances.\n",
    "\n",
    "-> KNN for Classification\n",
    "   When KNN is used for classification, the output can be calculated as the class with the highest frequency from the K-most     similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the             prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat are the different distances used in KNN? How are they calculated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, four different distance functions, which are Euclidean distance, cosine similarity measure, Minkowsky, correlation, and Chi square, are used in the k-NN classifier respectively.\n",
    "\n",
    "formula image:- https://miro.medium.com/max/700/1*Lh6R4QArolCRdzF7jjSzDw.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat are Lazy Learners? Why KNN is called a lazy learner?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lazylearning refers to any machine learning process that defers the majority of computation to consultation time. Two typical examples of lazy learning are instance-based learning and Lazy Bayesian Rules. Lazy learning stands in contrast to eager learning in which the majority of computation occurs at training time.\n",
    "\n",
    "K-NN is a lazy learner because it doesn't learn a discriminative function from the training data but “memorizes” the training dataset instead. For example, the logistic regression algorithm learns its model weights (parameters) during training time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tHow do we select the value of k? How bias and variance varies with k?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) There are no pre-defined statistical methods to find the most favorable value of K.\n",
    "2) Initialize a random K value and start computing.\n",
    "3) Choosing a small value of K leads to unstable decision boundaries.\n",
    "4) The substantial K value is better for classification as it leads to smoothening the decision boundaries.\n",
    "5) Derive a plot between error rate and K denoting values in a defined range. Then choose the K value as having a minimum error rate.\n",
    "\n",
    "The bias will be 0 when K=1, however, when it comes to new data (in test set), it has higher chance to be an error, which causes high variance. When we increase K, the training error will increase (increase bias), but the test error may decrease at the same time (decrease variance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)\tWhat are advantages and disadvantages of KNN?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Advantages of KNN\n",
    "\n",
    "1. No Training Period: KNN is called Lazy Learner (Instance based learning). It does not learn anything in the training period. It does not derive any discriminative function from the training data. In other words, there is no training period for it. It stores the training dataset and learns from it only at the time of making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g. SVM, Linear Regression etc.\n",
    "\n",
    "2. Since the KNN algorithm requires no training before making predictions, new data can be added seamlessly which will not impact the accuracy of the algorithm.\n",
    "\n",
    "3. KNN is very easy to implement. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)\n",
    "\n",
    "\n",
    "Disadvantages of KNN\n",
    "\n",
    "1. Does not work well with large dataset: In large datasets, the cost of calculating the distance between the new point and each existing points is huge which degrades the performance of the algorithm.\n",
    "\n",
    "2. Does not work well with high dimensions: The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate the distance in each dimension.\n",
    "\n",
    "3. Need feature scaling: We need to do feature scaling (standardization and normalization) before applying KNN algorithm to any dataset. If we don't do so, KNN may generate wrong predictions.\n",
    "\n",
    "4. Sensitive to noisy data, missing values and outliers: KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tDiscuss kDTree algorithm used for KNN."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "One of the most common algorithms that most of the Data scientists use for retrieval of information is KNN. K Nearest Neighbour (KNN) is one of the simplest algorithms that calculates the distance between the query observation and each data point in the train dataset and finds the K closest observations.\n",
    "\n",
    "We all know that, greater the data points, higher will be the amount of computation needed.\n",
    "\n",
    "So, KNN  search has O(N) time complexity for each query where N= Number of data points. For KNN with K neighbor search, the time complexity will be O(log(K)*N) only if we maintain a priority queue to return the closest K observations. You can read more about KNN here.\n",
    "\n",
    "So, for a dataset with millions of rows and thousands of queries, KNN seems to be computationally very demanding. So is there any alternative to KNN which uses similar approach but can be time efficient also?\n",
    "\n",
    "KD Tree is one such algorithm which uses a mixture of Decision trees and KNN to calculate the nearest neighbour(s).\n",
    "\n",
    "KD-trees are a specific data structure for efficiently representing our data. In particular, KD-trees helps organize and partition the data points based on specific conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "8)\tDiscuss Ball Tree algorithm used for KNN."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In KNN, all of the training data is stored in memory. When we have a new data point we need to find the nearest neighbors for that data point. For this reason, we need fast and optimized data structures where the search can be as fast as possible.\n",
    "\n",
    "In KNN, we have three major such implementations for storing all of the data, viz.\n",
    "\n",
    "-> Brute force approach\n",
    "-> K-D tree\n",
    "-> Ball Tree\n",
    "\n",
    "Ball Tree: Ball tree helps to overcome the inefficiencies of the KD tree for higher dimensions. Ball tree is much faster for storing multidimensional data. Ball tree partitions data in nested hyperspheres called balls. In ball tree, each node contains a hypersphere. And each hypersphere contains a subset of points which are needed to be searched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
