{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Regression?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).\n",
    "The two basic types of regression are simple linear regression and multiple linear regression.\n",
    "\n",
    "Simple linear regression uses one independent variable to explain or predict the outcome of the dependent variable Y, while multiple linear regression uses two or more independent variables to predict the outcome.\n",
    "\n",
    "The general form of each type of regression is:\n",
    "\n",
    "Simple linear regression: Y = a + bX + u\n",
    "Multiple linear regression: Y = a + b1X1 + b2X2 + b3X3 + ... + btXt + u\n",
    "Where:\n",
    "\n",
    "Y = the variable that you are trying to predict (dependent variable).\n",
    "X = the variable that you are using to predict Y (independent variable).\n",
    "a = the intercept.\n",
    "b = the slope.\n",
    "u = the regression residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is Linear Regression?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Linear regression quantifies the relationship between one or more predictor variable(s) and one outcome variable. Linear regression is commonly used for predictive analysis and modeling. For example, it can be used to quantify the relative impacts of age, gender, and diet (the predictor variables) on height (the outcome variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. When to use Linear Regression? Explain the equation of a straight line.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Linear regression models are used to show or predict the relationship between two variables or factors.The factor on which we are predicting the outcome (response) is called dependent variable and output variable is known as independent varaible.\n",
    "The dependent variable usually denoted as x.\n",
    "The equation of straight line is y=mx+c where,\n",
    "m is the gradient of the line and \n",
    "c is the y-intercept (where the graph crosses the y-axis). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat kind of plots will you use to showcase the relationship amongst the columns?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scatteer plots are the most widely used plots for plotting the two columns. It is used to determine the relationship between two variables.\n",
    "It is the most commonly used data visualization technique and helps in drawing useful insights when comparing two variables. The relationship between two variables is called correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tHow is the best fit line chosen?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A line of best fit can be roughly determined using an eyeball method by drawing a straight line on a scatter plot so that the number of points above the line and below the line is about equal (and the line passes through as many points as possible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat is gradient descent, and why is it used? Explain the maths."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gradient descent is nothing but a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.\n",
    "Gradient descent is simply used to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible. You start by defining the initial parameter's values and from there gradient descent uses calculus to iteratively adjust the values so they minimize the given cost-function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tWhat are residuals?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Residuals in a statistical or machine learning model are the differences between observed and predicted values of data. They are a diagnostic measure used when assessing the quality of a model. They are also known as errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is correlation?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Correlation is a term that is a measure of the strength of a linear relationship between two quantitative variables\n",
    "eg. (Height,weight)\n",
    "Positive correlation is a relationship between two variables in which both variables move in the same direction Whilst negative correlation is a relationship where one variable increases as the other decreases, and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat is multicollinearity?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Multicollinearity exists whenever two or more of the predictors in a regression model are moderately or highly correlated.\n",
    "or Multicollinearity occurs when independent variables in a regression model are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tHow to detect multicollinearity?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Multicollinearity can also be detected with the help of tolerance and its reciprocal, called variance inflation factor (VIF). If the value of tolerance is less than 0.2 or 0.1 and, simultaneously, the value of VIF 10 and above, then the multicollinearity is problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tWhat are the remedies for multicollinearity?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Following are the remedies for multicollinearity:\n",
    "\n",
    "1) Remove some of the highly correlated independent variables.\n",
    "2) Linearly combine the independent variables, such as adding them together.\n",
    "3) Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares    regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tWhat is the R-Squared Statistics?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tWhat is an adjusted R-Squared Statistics?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.\tWhy do we use adj R-squared?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Adjusted R-squared is used to determine how reliable the correlation is and how much is determined by the addition of independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.\tWhy adj R-squared decreases when we use incompetent variables?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.\tHow to interpret a Linear Regression model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In simple or multiple linear regression, the size of the coefficient for each independent variable gives you the size of the effect that variable is having on your dependent variable, and the sign on the coefficient (positive or negative) gives you the direction of the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.\tWhat is the difference between fit, fit_transform and predict methods?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fit performs the training, transform changes the data in the pipeline in order to pass it on to the next stage in the pipeline, and fit_transform does both the fitting and the transforming in one possibly optimized step.Prediction is the method used by estimators, it predicts values by learning from inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.\tHow do you plot the least squared line?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Our aim is to calculate the values m (slope) and b (y-intercept) in the equation of a line which is y=mx+c\n",
    "followong are theh steps to find the parameters and ultimately the \n",
    "\n",
    "Step 1: For each (x,y) point calculate x2 and xy\n",
    "\n",
    "Step 2: Sum all x, y, x2 and xy, which gives us Σx, Σy, Σx2 and Σxy (Σ means \"sum up\")\n",
    "\n",
    "Step 3: Calculate Slope m:\n",
    "\n",
    "m =  N Σ(xy) − Σx ΣyN Σ(x2) − (Σx)2 \n",
    "\n",
    "(N is the number of points.)\n",
    "\n",
    "Step 4: Calculate Intercept b:\n",
    "\n",
    "b =  Σy − m ΣxN \n",
    "\n",
    "Step 5: Assemble the equation of a line\n",
    "\n",
    "y = mx + b\n",
    "\n",
    "and then plot x and y which we have got by equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19.\tWhat are Bias and Variance? What is Bias Variance Trade-off?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. \n",
    "\n",
    "Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. \n",
    "\n",
    "If the algorithm is too simple (hypothesis with linear eq.) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex ( hypothesis with high degree eq.) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as Trade-off or Bias Variance Trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.\tWhat is the null and alternate hypothesis?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The null hypothesis states that a population parameter (such as the mean, the standard deviation, and so on) is equal to a hypothesized value. A null hypothesis is a hypothesis that says there is no statistical significance between the two variables in the hypothesis. It is the hypothesis that the researcher is trying to disprove\n",
    "\n",
    "The alternative hypothesis states that a population parameter is smaller, greater, or different than the hypothesized value in the null hypothesis.An alternative hypothesis simply is the inverse, or opposite, of the null hypothesis.It is the hypothesis that the researcher is trying to prove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21.\tWhat is multiple linear regression?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Multiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. \n",
    "The goal of multiple linear regression (MLR) is to model the linear relationship between the explanatory (independent) variables and one or more response (dependent) variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22.\tWhat is the OLS method? Derive the formulae used in the OLS method."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In statistics, ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23.\tWhat is the p-value? How does it help in feature selection?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In statistics, the p-value is the probability of obtaining results as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is correct. ... A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.\n",
    "\n",
    "Removal of different features from the dataset will have different effects on the p-value for the dataset. We can remove different features and measure the p-value in each case. These measured p-values can be used to decide whether to keep a feature or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24.\tHow to handle categorical values in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25.\tWhat is regularization, and why do we need it?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Regularization is a technique used for tuning the function by adding an additional penalty term in the error function. The additional term controls the excessively fluctuating function such that the coefficients don't take extreme values.\n",
    "\n",
    "Regularization. This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.\tExplain Ridge Regression."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When\n",
    "multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from\n",
    "the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27.\tExplain Lasso Regression."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.\tExplain Elastic Net."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.\tWhy do we do a train test split?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To avoid overfitting. Sometimes when you train on data you can get models that works great for the exact data you have trained on, but not other data. It's basically to see if the model you made is sound. The usual way of thinking in machine learning is to split the data in train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30.\tWhat is polynomial regression? When to use it?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this regression, the relationship between dependent and the independent variable is modeled such that the dependent variable Y is an nth degree function of independent variable Y.\n",
    "The polynomial regression fits into a non-linear relationship between the value of X and the value of Y.\n",
    "The Polynomial regression is also called as multiple linear regression models.The Polynomial regression model has been an important source for the development of regression analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.\tExplain the steps for GCP deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. What difficulties did you face in cloud deployment?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
