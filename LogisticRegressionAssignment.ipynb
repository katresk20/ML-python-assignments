{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is a classification problem?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In machine learning, classification refers to a predictive modeling problem where a class label is predicted for a given example of input data.\n",
    "\n",
    "Examples of classification problems include:\n",
    "\n",
    "1) Given an example, classify if it is spam or not.\n",
    "2) Given recent user behavior, classify as churn or not.\n",
    "\n",
    "From a modeling perspective, classification requires a training dataset with many examples of inputs and outputs from which to learn.\n",
    "\n",
    "There are perhaps four main types of classification tasks that you may encounter; they are:\n",
    "\n",
    "Binary Classification\n",
    "Multi-Class Classification\n",
    "Multi-Label Classification\n",
    "Imbalanced Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is Logistic Regression?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logistic Regressipn is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhy is the Sigmoid function used for Logistic Regression?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The reason we choose a sigmoid function to model classification problems solved with logistic regression is that we want to make sure that predicted value has a defined range which aids us in differentiating the classes. This has proved to be very useful in binary classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat is the cost function?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is a function that measures the performance of a Machine Learning model for given data. Cost Function quantifies the error between predicted values and expected values and presents it in the form of a single real number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tWhat is multiple Logistic Function?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Multiple Logistic Regression Analysis. ... Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat is multilabel Logistic Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tHow does the Logistic Regression Algorithm learn?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes. ... Mathematically, a logistic regression model predicts P(Y=1) as a function of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tExplain the confusion matrix."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n",
    "\n",
    "The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made.\n",
    "\n",
    "\n",
    "\t\n",
    "                       Class 1 (Predicted)\t   Class 2 (Predicted)\n",
    "                                               \n",
    "Class 1 (Actual)\t      True Positive           False Negative\n",
    "                        \t\n",
    "\n",
    "Class 2 (Actual)          False Positive\t      True Negative\n",
    "\t\n",
    "\n",
    "Positive (P) : Observation is positive (for example: is an apple).\n",
    "Negative (N) : Observation is not positive (for example: is not an apple).\n",
    "True Positive (TP) : Observation is positive, and is predicted to be positive.\n",
    "False Negative (FN) : Observation is positive, but is predicted negative.\n",
    "True Negative (TN) : Observation is negative, and is predicted to be negative.\n",
    "False Positive (FP) : Observation is negative, but is predicted positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat is Accuracy?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy has the following definition:\n",
    "\n",
    "Accuracy=\n",
    "Number of correct predictions\n",
    "Total number of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tWhy there is a need for other metrics if ‘accuracy’ is already present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tWhat are Recall and Precision? How do they differ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precision and recall are two extremely important model evaluation metrics. While precision refers to the percentage of your results which are relevant, recall refers to the percentage of total relevant results correctly classified by your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tHow does the tradeoff between recall and precision work?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "precision-recall tradeoff occur due to increasing one of the parameter(precision or recall) while keeping the model same. ... When threshold is decreased to -200000, recall increases to 0.95 but precision decreases to 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tExplain the F1 score."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The F score, also called the F1 score or F measure, is a measure of a test's accuracy. ... The F score reaches the best value, meaning perfect precision and recall, at a value of 1. The worst F score, which means lowest precision and lowest recall, would be a value of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.\tWhat is specificity?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Formally, Specificity is the proportion of truly negative cases that were classified as negative; thus, it is a measure of how well your classifier identifies negative cases. It is also known as the true negative rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.\tExplain the significance of ROC."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ROC curves are frequently used to show in a graphical way the connection/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests. In addition the area under the ROC curve gives an idea about the benefit of using the test(s) in question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.\tWhat is AUC, and when is it used?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n",
    "\n",
    "It is used in classification analysis in order to determine which of the used models predicts the classes best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.\tExplain the steps for Heroku deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.\tWhat difficulties did you face while deploying to Heroku?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
