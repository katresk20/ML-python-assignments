{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tExplain the curse of dimensionality."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to the phenomena that occur when classifying, organizing, and analyzing high dimensional data that does not occur in low dimensional spaces, specifically the issue of data sparsity and “closeness” of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is a dimensionality reduction technique?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dimensionality reduction refers to techniques for reducing the number of input variables in training data.\n",
    "\n",
    "When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the “essence” of the data. This is called dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tExplain PCA. What are the principal components?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n",
    "\n",
    "So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible.\n",
    "\n",
    "\n",
    "Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. ... Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat is explained variance ratio?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The fraction of variance explained by a principal component is the ratio between the variance of that principal component and the total variance. For several principal components, add up their variances and divide by the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tWhat is a scree plot?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A scree plot is a graphical tool used in the selection of the number of relevant components or factors to be considered in a principal components analysis or a factor analysis.The scree plot is a way of visualizing the magnitude of the variability associated with each one of the components extracted in a principal component analysis. This plot allows researchers to examine the pattern of decreasing variability attributable to each successive component in order to inform the selection of how many such components should be considered.\n",
    "\n",
    "The scree plot is a way of visualizing the magnitude of the variability associated with each one of the components extracted in a principal component analysis. This plot allows researchers to examine the pattern of decreasing variability attributable to each successive component in order to inform the selection of how many such components should be considered ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tHow is the optimum number of principal components obtained?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The minimization of the unreconstructed variance will determine the optimal number of principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tWhat is covariance?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is the transpose of a matrix?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The transpose of a matrix is simply a flipped version of the original matrix. We can transpose a matrix by switching its rows with its columns. We denote the transpose of matrix A by AT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat is an Eigen value and Eigen Vector?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Eigen vector of a matrix A is a vector represented by a matrix X such that when X is multiplied with matrix A, then the direction of the resultant matrix remains same as vector X.\n",
    "\n",
    "Mathematically, above statement can be represented as:\n",
    "\n",
    "AX = λX\n",
    "\n",
    "where A is any arbitrary matrix, λ are eigen values and X is an eigen vector corresponding to each eigen value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tWhy the Principal Components are orthogonal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tExplain the Eigen Decomposition approach."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In linear algebra, eigendecomposition or sometimes spectral decomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tHow can PCA be used for data compression?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Principal component analysis can be considered from the viewpoint of data compression. The idea behind this is that by reducing the number of eigenvectors used to reconstruct the original data matrix, the amount of required storage space is reduced. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tDiscuss the pros and cons of PCA."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Following are some of the advantages and disadvantages of Principal Component Analysis:\n",
    "\n",
    "Advantages of Principal Component Analysis\n",
    "\n",
    "1. Removes Correlated Features: In a real world scenario, this is very common that you get thousands of features in your dataset. You cannot run your algorithm on all the features as it will reduce the performance of your algorithm and it will not be easy to visualize that many features in any kind of graph. So, you MUST reduce the number of features in your dataset. \n",
    "\n",
    "2. Improves Algorithm Performance: With so many features, the performance of your algorithm will drastically degrade. PCA is a very common way to speed up your Machine Learning algorithm by getting rid of correlated variables which don't contribute in any decision making. The training time of the algorithms reduces significantly with less number of features.\n",
    "\n",
    "3. Reduces Overfitting: Overfitting mainly occurs when there are too many variables in the dataset. So, PCA helps in overcoming the overfitting issue by reducing the number of features.\n",
    "\n",
    "4. Improves Visualization: It is very hard to visualize and understand the data in high dimensions. PCA transforms a high dimensional data to low dimensional data (2 dimension) so that it can be visualized easily. \n",
    "\n",
    "\n",
    "Disadvantages of Principal Component Analysis\n",
    "\n",
    "1. Independent variables become less interpretable: After implementing PCA on the dataset, your original features will turn into Principal Components. Principal Components are the linear combination of your original features. Principal Components are not as readable and interpretable as original features.\n",
    "\n",
    "2. Data standardization is must before PCA: You must standardize your data before implementing PCA, otherwise PCA will not be able to find the optimal Principal Components. \n",
    "\n",
    "3. Information Loss: Although Principal Components try to cover maximum variance among the features in a dataset, if we don't select the number of Principal Components with care, it may miss some information as compared to the original list of features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
